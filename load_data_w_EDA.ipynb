{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d2fbb29",
   "metadata": {},
   "source": [
    "This notebook is designed to be run as the first notebook of four included in this submission.\n",
    "\n",
    "# Hailey Weinschenk - BrainStation Capstone Notebook 1 - Data Loading and Explatory Data Analysis\n",
    "\n",
    "In this notebook, the data will be loaded and prepared for later modelling. Then, some basic EDA will be performed such as null-handling, viewing distributions, and correlations. First, import necessary libraries and load the data. If the file exists, only the else clause is activated which causes about a 10 minute load time. Without the file, the if clause takes 4-5 hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebac9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from preprocess import preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d24fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.exists('full_preprocessedML.csv')):\n",
    "    x_dim = 216\n",
    "    y_dim = 288\n",
    "    deep = False\n",
    "    df = preprocess(x_dim,y_dim,deep)\n",
    "    df.to_csv('full_preprocessedML.csv')\n",
    "else:\n",
    "    df_chunk = pd.read_csv('full_preprocessedML.csv',index_col=0,iterator=True,chunksize=250)\n",
    "    df = pd.concat(df_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee43d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8b312",
   "metadata": {},
   "source": [
    "Firstly, the shape of the read-in data is 2756 rows by 186631 columns. This can be interpreted as 2756 images in the database, each of which has about 186,000 features (each pixel). There are, of course, the features of 'is_red', 'suit_num' and 'card_number' as well as helper columns to obtain them. Even with these disregarded, our column space is massive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991245bb",
   "metadata": {},
   "source": [
    "Now, the distribution of the features will be discussed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c631ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby('card_string').count().iloc[:,1]\n",
    "counts.sort_values(ascending = False).head(),counts.sort_values(ascending = True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171874e0",
   "metadata": {},
   "source": [
    "So all cards have between 51 and 52 images each. Interestingly, the 4 of clubs has 10 extra images. This could be an error when creating the database. Regardless, this shouldn't have a large impact on our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby('suit').count().iloc[:,1]\n",
    "plt.figure()\n",
    "plt.title('Suit Distributions')\n",
    "plt.xlabel('Suits')\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(counts.keys(),counts.values, color = ['green','blue','red','yellow','black'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ef96f",
   "metadata": {},
   "source": [
    "Our suit distributions are to be expected. Each suit other than clubs has $13*51\\approx(663)$ images. Clubs has a few extra, due to the extra 4 of clubs images. Jokers simply has one rank so therefore only 51 or 52 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd35483",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby('is_red').count().iloc[:,1]\n",
    "plt.figure()\n",
    "plt.title('Binary Distributions')\n",
    "plt.xlabel('Color')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(ticks = [0,1],labels = ['Black','Red'])\n",
    "plt.bar(counts.keys(),counts.values,color = ['black','red'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc72c6",
   "metadata": {},
   "source": [
    "Finally, we see that this increase in the 4 of clubs effected the black/red distribution as well. Additionally, differences in 51 vs. 52 for more black cards could appreciate the difference shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758b932",
   "metadata": {},
   "source": [
    "## Null Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ed620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.isna().sum() == 0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee663f49",
   "metadata": {},
   "source": [
    "So our each of the entire (186631!) columns is full of zeroes. Therefore, there are no null values present in this dataset. This is a clear advantage of working with a 'toy' problem or image data in general. However, the difficulty comes from managing and manipulating such inconcievably large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c5e1d",
   "metadata": {},
   "source": [
    "With this in mind, the correlation coefficients of each feature can be obtained. Since 186 thousand features would be impossible to view, we will take a small slice of our data and view the correlations. The goal of this is to show potential multicolinearity that can cause issues when modelling. This occurs when features have relationships with other features as well as on the target. It is necessary to deal with these in some way before moving on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_df = df.iloc[1:50,15000:15025]\n",
    "sns.heatmap(part_df.corr(),cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3745a6",
   "metadata": {},
   "source": [
    "Despite some high correlation scores throughout the heatmap, there is no clear pattern of high or low between two pixels for *each* picture. One image might have a dark object featured, which would cause a high negative correlation to adjacent pixels for that image. \n",
    "\n",
    "With this in mind, we can move forward with modelling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
